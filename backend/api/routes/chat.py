# Streaming chat

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse
import json
import asyncio
from typing import AsyncGenerator

from api.schemas.requests import ChatRequest
from api.schemas.responses import ChatResponse, Citation
from core.config import AppConfig
from core.logging import get_logger
from ingestion.embed_store import load_faiss
from retrieval.search import as_retriever
from rag.chain import build_rag_chain, postprocess_citations

router = APIRouter(prefix="/api/chat", tags=["Chat"])
logger = get_logger()
config = AppConfig()


def get_vectorstore():
    """Dependency to load vector store"""
    try:
        return load_faiss(str(config.db_dir), embedding_model=config.embedding_model)
    except Exception as e:
        raise HTTPException(500, f"Vector store not initialized: {str(e)}")


@router.get("/stream")
async def stream_chat(
    query: str,
    llm_model: str = None,
    temperature: float = None,
    top_k: int = None,
    vs = Depends(get_vectorstore)
):
    """
    Streaming chat endpoint using Server-Sent Events (SSE)
    
    Streams tokens in real-time as they're generated by the LLM
    """
    
    # Use request-level overrides or fall back to config
    llm_model = llm_model or config.llm_model
    temperature = temperature if temperature is not None else config.temperature
    top_k = top_k or config.top_k
    
    # Validate model
    if not config.validate_llm_model(llm_model):
        raise HTTPException(400, f"Invalid model: {llm_model}")
    
    logger.info(f"Streaming query with model={llm_model}, temp={temperature}, top_k={top_k}")
    
    # Build retriever and chain
    retriever = as_retriever(vs, k=top_k, fetch_k=config.fetch_k, use_mmr=config.use_mmr)
    rag_chain = build_rag_chain(retriever, llm_model=llm_model, temperature=temperature)
    
    async def event_generator() -> AsyncGenerator[str, None]:
        """Generate SSE events..."""
        try:
            # Get retrieved documents first
            raw_docs = retriever.invoke(query)
            citations = postprocess_citations(raw_docs)
            
            # Send citations first
            yield {
                "event": "citations",
                "data": json.dumps([c.__dict__ if hasattr(c, '__dict__') else c for c in citations])
            }
            
            # Stream answer tokens
            full_answer = ""
            for chunk in rag_chain.stream({"input": query}):
                if isinstance(chunk, dict) and "answer" in chunk:
                    token = chunk["answer"]
                elif isinstance(chunk, str):
                    token = chunk
                else:
                    continue
                
                full_answer += token
                yield {
                    "event": "token",
                    "data": json.dumps({"token": token})
                }
                
                # Small delay to prevent overwhelming client
                await asyncio.sleep(0.01)
            
            # Send completion event
            yield {
                "event": "done",
                "data": json.dumps({"model": llm_model})
            }
            
        except Exception as e:
            logger.error(f"Streaming error: {e}")
            yield {
                "event": "error",
                "data": json.dumps({"error": str(e)})
            }
    
    return EventSourceResponse(event_generator())


@router.post("/", response_model=ChatResponse)
async def chat(request: ChatRequest, vs = Depends(get_vectorstore)):
    """
    Non-streaming chat endpoint (fallback)
    
    Returns complete response with citations
    """
    
    llm_model = request.llm_model or config.llm_model
    temperature = request.temperature if request.temperature is not None else config.temperature
    top_k = request.top_k or config.top_k
    
    if not config.validate_llm_model(llm_model):
        raise HTTPException(400, f"Invalid model: {llm_model}")
    
    try:
        # Build chain
        retriever = as_retriever(vs, k=top_k, fetch_k=config.fetch_k, use_mmr=config.use_mmr)
        rag_chain = build_rag_chain(retriever, llm_model=llm_model, temperature=temperature)
        
        # Get raw documents for citations
        raw_docs = retriever.invoke(request.query)
        
        # Invoke chain
        result = rag_chain.invoke({"input": request.query})
        
        # Extract answer (handle different response formats)
        if isinstance(result, dict):
            answer = result.get("answer", str(result))
        else:
            answer = str(result)
        
        # Process citations
        citations = postprocess_citations(raw_docs)
        citations_list = [Citation(**c) for c in citations]
        
        return ChatResponse(
            answer=answer,
            citations=citations_list,
            model_used=llm_model,
            query=request.query
        )
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(500, f"Chat failed: {str(e)}")
